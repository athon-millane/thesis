{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gene2Vec Experiments\n",
    "Attempting to recreate results of [gene2vec paper](https://doi.org/10.1186/s12864-018-5370-x)\n",
    "\n",
    "1. [Import known gene pathways from MSigDB](#pathways)\n",
    "2. [Generate pairs of genes from gene pathway sets](#pairs)\n",
    "3. [Use `gensim` to train with gene2vec](#gensim)\n",
    "4. [Visualise embeddings with PCA and tSNE](#vis)\n",
    "5. [Load and preprocess somatic mutations from TCGA in BigQuery](#tcga)\n",
    "6. [Produce 2D images for every sample](#2d)\n",
    "7. [Model Training](#training) \n",
    "    - 7.1 [7.1 Ensemble of sklearn models on flat data (baseline on sparse samples)](#sklearn)\n",
    "    - 7.2 [Random Forest on images](#rf)\n",
    "    - 7.3 [ResNet50 w ImageNet weight initialisation (FastAI)](#fastai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Initialisation, Imports, Environment Variables\n",
    "<a id=\"pathways\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Environment configuration - uncomment to run remotely ###\n",
    "\n",
    "# # install conda\n",
    "# !wget -c https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh\n",
    "# !chmod +x Anaconda3-5.1.0-Linux-x86_64.sh\n",
    "# !bash ./Anaconda3-5.1.0-Linux-x86_64.sh -b -f -p /usr/local\n",
    "\n",
    "# # import sys\n",
    "# sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
    "\n",
    "# # set up file system\n",
    "# !git clone https://athon-millane:$GITHUB_TOKEN@github.com/athon-millane/thesis.git\n",
    "# !cd thesis && mkdir -p data/{gene2vec,mutsigcv} && mkdir -p /experiments/gene2vec/models\n",
    "\n",
    "# # initalise and activate thesis conda environment\n",
    "# !cd thesis && conda env create -f environment_nb.yml\n",
    "# !source activate thesis\n",
    "\n",
    "# # add environment to path\n",
    "# import sys\n",
    "# sys.path.append('/usr/local/envs/thesis/lib/python3.6/site-packages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, sys, time, datetime, math, random\n",
    "\n",
    "# dataproc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt, matplotlib.image as img\n",
    "\n",
    "# gene2vec\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import PCA\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# sklearn\n",
    "# import sklearn.ensemble as ske\n",
    "# from sklearn import datasets, model_selection, tree, preprocessing, metrics, linear_model\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier \n",
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, SGDClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# pytorch and fastai\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word2vec parameters\n",
    "W2V_DIMENSION   = 256   # dimension of the embedding\n",
    "W2V_NUM_WORKERS = 96    # number of worker threads\n",
    "W2V_MIN_COUNT   = 1     # ignores all words with total frequency lower than this\n",
    "W2V_SG          = 1     # sg =1, skip-gram, sg =0, CBOW\n",
    "W2V_N_ITER      = 10    # number of iterations\n",
    "W2V_WINDOW_SIZE = 1     # The maximum distance between the gene and predicted gene within a gene list\n",
    "W2V_NEGATIVE    = 3     # number of negative samples\n",
    "\n",
    "# Environment variables\n",
    "MSIGDB          = '../data/gene2vec/msigdb.v6.2.symbols.gmt'\n",
    "GENE_PAIRS      = '../data/gene2vec/gene_pairs_set.pkl'\n",
    "GENE2VEC_DIR    = '../models/gene2vec/'\n",
    "TCGA            = '../data/tcga/processed_somatic_mutations_subset2'\n",
    "INTOGEN         = '../data/tcga/intogen-drivers-data.tsv'\n",
    "EXP1_DIR        = '../models/sklearn-ensemble/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions #\n",
    "\n",
    "def size(obj, suffix='B'):\n",
    "    \"\"\"\n",
    "    Get the size of provided object in human readable format.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    num = sys.getsizeof(obj)\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "def freememory():\n",
    "    \"\"\"\n",
    "    Run garbage collection to free up memory.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import known gene pathways from [MSigDB](http://software.broadinstitute.org/gsea/msigdb/collections.jsp)\n",
    "<a id=\"pathways\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_data(line):\n",
    "    \"\"\"\n",
    "    Read pathway name and set of genes from each line, parsing gene appropriately.\n",
    "    \"\"\"\n",
    "    name, _, genes = line.split(\"\\t\", 2)\n",
    "    return name, set(gene.replace('\\n', '') for gene in genes.split(\"\\t\"))\n",
    "\n",
    "def load_msigdb():\n",
    "    \"\"\"\n",
    "    Read msigdb pathways file line by line and parse into useful dataframe.\n",
    "    \"\"\"\n",
    "    with open(MSIGDB, 'r') as fp:\n",
    "        pathways_df = pd.DataFrame(data=[_get_data(line) for line in fp],\n",
    "                                   columns=[\"name\", \"set\"])\n",
    "        \n",
    "    return pathways_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathways_df = load_msigdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate pairs of genes from gene pathway sets\n",
    "<a id=\"pairs\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gene_pairs(gene_set):\n",
    "    \"\"\"\n",
    "    Get gene pairs\n",
    "    \"\"\"\n",
    "    from itertools import combinations\n",
    "    # Get all combinations of length 2 \n",
    "    pairs = combinations(list(gene_set), 2)\n",
    "    return set(pairs)\n",
    "\n",
    "def get_pairs_set(df):\n",
    "    \"\"\"\n",
    "    Return the set union of all pairs from every gene set within the df.\n",
    "    \"\"\"\n",
    "    return set.union(*df['pairs'].tolist())\n",
    "\n",
    "def load_pairs():\n",
    "    \"\"\"\n",
    "    If gene pairs file exists, load it. Otherwise, create it and save to gene pairs file.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(GENE_PAIRS):\n",
    "        print(\"Reading gene pairs file.\")\n",
    "        with open (GENE_PAIRS, 'rb') as f:\n",
    "            gene_pairs = pkl.load(f)\n",
    "    else:\n",
    "        print(\"Creating new gene pairs file.\")\n",
    "        pathways_df['pairs'] = (pathways_df.set\n",
    "                                           .apply(generate_gene_pairs))\n",
    "        gene_pairs = pathways_df.pipe(get_pairs_set)\n",
    "\n",
    "        with open (GENE_PAIRS, 'wb') as f:\n",
    "            pkl.dump(gene_pairs, f)\n",
    "            \n",
    "    return gene_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading gene pairs file.\n",
      "CPU times: user 2min 53s, sys: 13.2 s, total: 3min 6s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gene_pairs = load_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "freememory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use [`gensim`](https://radimrehurek.com/gensim/models/word2vec.html) to train with gene2vec\n",
    "<a id=\"gene2vec\"></a>\n",
    "- With a c compile installed `gensim` offers a [70x speedup compared to plain NumPy implementation](https://rare-technologies.com/parallelizing-word2vec-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(gene_pairs, dimension=W2V_DIMENSION):\n",
    "    \"\"\"\n",
    "    Trains n_iter iterations of word2vec and saves model on each iteration.\n",
    "    \"\"\"\n",
    "    # initialise and save model\n",
    "    model = gensim.models.Word2Vec(gene_pairs, \n",
    "                                   size=dimension,\n",
    "                                   window=W2V_WINDOW_SIZE, \n",
    "                                   min_count=W2V_MIN_COUNT, \n",
    "                                   workers=W2V_NUM_WORKERS, \n",
    "                                   negative=W2V_NEGATIVE,\n",
    "                                   sg=W2V_SG)\n",
    "    print(\"gene2vec model initialised\")\n",
    "    \n",
    "    current_iter = 0\n",
    "    filename = 'dim_{}/iter_{}'.format(dimension, current_iter)\n",
    "    \n",
    "    model.save(GENE2VEC_DIR + filename)\n",
    "    print(\"gene2vec model saved\")\n",
    "    del model \n",
    "    \n",
    "    # train model\n",
    "    for current_iter in range(1,W2V_N_ITER+1):\n",
    "        loadfile = 'dim_{}/iter_{}'.format(dimension, current_iter - 1)\n",
    "        \n",
    "        # shuffle pairs\n",
    "        random.shuffle(gene_pairs)\n",
    "        \n",
    "        # load model and train on shuffled gene pairs\n",
    "        model = gensim.models.Word2Vec.load(GENE2VEC_DIR + filename)\n",
    "        model.train(gene_pairs,total_examples=model.corpus_count,epochs=model.iter)\n",
    "        \n",
    "        # save trained model\n",
    "        savefile = 'dim_{}/iter_{}'.format(dimension, current_iter)\n",
    "        model.save(GENE2VEC_DIR + savefile)\n",
    "        print(\"gene2vec dimension \" + str(dimension) + \" iteration \" + str(current_iter) + \" saved\")\n",
    "        del model\n",
    "        \n",
    "def train_word2vec_gridsearch(gene_pairs):\n",
    "    \"\"\"\n",
    "    Trains a gridsearch (entirety of provided parameter space) of different\n",
    "    word2vec models, and saves in directory tree.\n",
    "    \"\"\"\n",
    "    dims = [16, 32, 64, 128, 256, 512, 1024]\n",
    "    \n",
    "    for dim in dims:\n",
    "        train_word2vec(gene_pairs, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-14aeab4d2ab8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_word2vec_gridsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgene_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-fce9daece83b>\u001b[0m in \u001b[0;36mtrain_word2vec_gridsearch\u001b[0;34m(gene_pairs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtrain_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgene_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-fce9daece83b>\u001b[0m in \u001b[0;36mtrain_word2vec\u001b[0;34m(gene_pairs, dimension)\u001b[0m\n\u001b[1;32m     10\u001b[0m                                    \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mW2V_NUM_WORKERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                    \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mW2V_NEGATIVE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                    sg=W2V_SG)\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gene2vec model initialised\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_train_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             self.train(\n\u001b[1;32m    337\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \"\"\"\n\u001b[1;32m    479\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m--> 480\u001b[0;31m             sentences, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         report_values = self.vocabulary.prepare_vocab(\n",
      "\u001b[0;32m~/.conda/envs/thesis/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1163\u001b[0m                     \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m                 )\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_word2vec_gridsearch(list(gene_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualise embeddings with PCA and tSNE\n",
    "<a id=\"vis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and train on shuffled gene pairs\n",
    "\n",
    "filename = GENE2VEC_DIR + 'dim_256/iter_9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_name):\n",
    "    model = KeyedVectors.load(file_name)\n",
    "    wordVector = model.wv\n",
    "    vocabulary, wv = zip(*[[word, wordVector[word]] for word, vocab_obj in wordVector.vocab.items()])\n",
    "    \n",
    "    return np.asarray(wv), vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9094bc9bdb70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtopN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for now select all genes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrdWV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mrdVB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtopN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# load test file\n",
    "wv, vocabulary = load_embeddings(filename)\n",
    "\n",
    "# shuffle index of genes\n",
    "indexes = list(range(len(wv)))\n",
    "random.shuffle(indexes)\n",
    "\n",
    "topN = len(wv) # for now select all genes\n",
    "rdWV = wv[indexes,:,:]\n",
    "rdVB = np.array(vocabulary)[indexes][:topN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.shape, rdWV.shape, rdVB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d(data, title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel('dimension 1', fontsize = 15)\n",
    "    ax.set_ylabel('dimension 2', fontsize = 15)\n",
    "    ax.set_title(title, fontsize = 20)\n",
    "    \n",
    "    ax.scatter(data[:,0], data[:,1], s = 5, alpha=0.5)\n",
    "    ax.grid()\n",
    "\n",
    "# 2 component PCA visualisation\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_2.fit(rdWV)\n",
    "pca_rdWV_2=pca_2.transform(rdWV)\n",
    "\n",
    "# plot\n",
    "plot_2d(pca_rdWV_2, title='2 Component PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 component PCA for tSNE\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(rdWV)\n",
    "pca_rdWV=pca.transform(rdWV)\n",
    "\n",
    "n_iters = [100,500,1000]\n",
    "# n_iters = [100,500,1000,10000,100000]\n",
    "\n",
    "def tsne_worker(n_iter):\n",
    "    \"\"\"\n",
    "    TSNE Worker, will run for CPU or GPU.\n",
    "    \"\"\"\n",
    "    tsne = TSNE(n_components=2, \n",
    "                perplexity=30, \n",
    "                n_iter=n_iter, \n",
    "                learning_rate=200, \n",
    "                n_jobs=8)\n",
    "    \n",
    "    print('n_iter = {0:d} started'.format(n_iter))\n",
    "    data = tsne.fit_transform(pca_rdWV)\n",
    "    print('n_iter = {0:d} finished'.format(n_iter))\n",
    "    return data, n_iter\n",
    "\n",
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print(\"CUDA available, using GPU TSNE\")\n",
    "    from tsnecuda import TSNE\n",
    "    for n_iter in n_iters:\n",
    "        tsne = TSNE(n_components=2, \n",
    "                perplexity=30, \n",
    "                n_iter=n_iter, \n",
    "                learning_rate=200)\n",
    "        data = tsne.fit_transform(pca_rdWV)\n",
    "        plot_2d(data, '2D tSNE, n_iter={0:d}'.format(n_iter))    \n",
    "else:\n",
    "    print(\"CUDA not available, using multi-core TSNE\")\n",
    "    from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "    \n",
    "    p = Pool(4)\n",
    "\n",
    "    # generate tsne of different iteration in parallel\n",
    "    results = p.map(tsne_worker, [100, 500])\n",
    "\n",
    "    # plot tSNE\n",
    "    for data, n_iter in results:\n",
    "        plot_2d(data, '2D tSNE, n_iter={0:d}'.format(n_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 5. Load and preprocess somatic mutations from [TCGA in BigQuery]\n",
    "(https://bigquery.cloud.google.com/table/isb-cgc:TCGA_hg38_data_v0.Somatic_Mutation?pli=1)\n",
    "<a id=\"tcga\"></a>\n",
    "\n",
    "- Subset query has already been completed in BQ and saved to Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcga_raw = pd.read_csv(TCGA)\n",
    "tcga_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_distributions(df, title):\n",
    "    \"\"\"\n",
    "    Plot distribution and frequency of features of interest for raw and processed TCGA df.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize = (20,8))\n",
    "    fig.suptitle(title)\n",
    "    plt.subplots_adjust(hspace=0.6)\n",
    "    \n",
    "    df.groupby('case_barcode').head(1)['project_short_name'].value_counts() \\\n",
    "        .plot(kind='bar', title='Cases per Cancer type', ax=axes[0,0], color='m')\n",
    "    \n",
    "    df['Variant_Classification'] \\\n",
    "        .value_counts().plot(kind='bar', title='Variants per variant type', ax=axes[1,0], logy=True, color='g')\n",
    "    \n",
    "    df['case_barcode'].value_counts() \\\n",
    "        .plot(title='Log Variants per case, {0:d} cases'\n",
    "              .format(df['case_barcode'].value_counts().shape[0]), \n",
    "              ax=axes[0, 1],  logy=True, color='r')\n",
    "    \n",
    "    df['Hugo_Symbol'].value_counts() \\\n",
    "        .plot(title='Log Variants per gene, {0:d} genes'\n",
    "              .format(df['Hugo_Symbol'].value_counts().shape[0]), \n",
    "              ax=axes[1, 1], logy=True, color='b')\n",
    "    \n",
    "visualise_distributions(tcga_raw, 'Raw TCGA Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In absence of MutSigCV results use list of 459 driver genes from [Intogen](https://www.intogen.org/search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_low_mut_count(df, feature, cutoff=100):\n",
    "    \"\"\"\n",
    "    Drop rows which contain features which occur less than cutoff times in the dataset.\n",
    "    \"\"\"\n",
    "    subsample = df[feature].value_counts()[(df[feature].value_counts() > cutoff)].index.tolist()\n",
    "    return df[df[feature].isin(subsample)]\n",
    "\n",
    "def merge_label(df, label1, label2, merged_label):\n",
    "    \"\"\"\n",
    "    Merge label1 and label2 into merged label within dataframe.\n",
    "    \"\"\"\n",
    "    df.loc[(df['project_short_name'] == label1) | \n",
    "           (df['project_short_name'] == label2), 'project_short_name'] = merged_label\n",
    "    return df\n",
    "\n",
    "def process_labels(df):\n",
    "    \"\"\"\n",
    "    Merge cancers that are established clinically to be the same.\n",
    "    \"\"\"\n",
    "    # Colon and Rectal cancers are now considered the same cancer\n",
    "    # COAD, READ -> COADREAD\n",
    "    df = merge_label(df, 'TCGA-COAD', 'TCGA-READ', 'MERGE-COADREAD')\n",
    "    \n",
    "    # GBM and LGG are both forms of brain Glioma\n",
    "    # GBM, LGG   -> GBMLGG\n",
    "    df = merge_label(df, 'TCGA-GBM', 'TCGA-LGG', 'MERGE-GBMLGG')\n",
    "    \n",
    "    # Stomach and Esophegal cancers are also considered the same\n",
    "    # ESCA, STAD -> STES\n",
    "    df = merge_label(df, 'TCGA-ESCA', 'TCGA-STAD', 'MERGE-STES')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_intogen_drivers(df):\n",
    "    \"\"\"\n",
    "    Filter only genes that intersect with listed drivers from Intogen.\n",
    "    \"\"\"\n",
    "    intogen_drivers = pd.read_csv(INTOGEN, sep='\\t')\n",
    "    driver_genes = intogen_drivers['SYMBOL'].tolist()\n",
    "    return df[df['Hugo_Symbol'].isin(driver_genes)]\n",
    "\n",
    "def filter_variants(df):\n",
    "    \"\"\"\n",
    "    Filter out variants according to a list provided by Dr Nic Waddel (QIMR).\n",
    "    \"\"\"\n",
    "    \n",
    "    waddell_list = ['missense_variant',\n",
    "                    'stop_gained',\n",
    "                    'frameshift_variant',\n",
    "                    'splice_acceptor_variant',\n",
    "                    'splice_donor_variant',\n",
    "                    'start_lost',\n",
    "                    'inframe_deletion',\n",
    "                    'inframe_insertion',\n",
    "                    'stop_lost']\n",
    "    \n",
    "    return df[df['One_Consequence'].isin(waddell_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empirical dim reduction\n",
    "df_proc1 = (tcga_raw.pipe(process_labels)\n",
    "                    .pipe(filter_variants)\n",
    "                    .pipe(filter_intogen_drivers))\n",
    "\n",
    "# statistical dim reductions\n",
    "df_proc2 = (tcga_raw.pipe(process_labels)\n",
    "                    .pipe(filter_variants)\n",
    "                    .pipe(drop_low_mut_count, 'Hugo_Symbol', 200))     # naïvely remove very genes with few mutations as noise\n",
    "\n",
    "visualise_distributions(df_proc1, 'Empirical Dim Reduction')\n",
    "visualise_distributions(df_proc2, 'Statistical Dim Reduction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_and_get_variant_count(df_in):\n",
    "    \"\"\"\n",
    "    Deduplicate gene sample combinations with >1 mutations and aggregate \n",
    "    with additional feature of variant count for gene sample combination.\n",
    "    \"\"\"\n",
    "    df = df_in.copy()\n",
    "    \n",
    "    counts = df.groupby('case_barcode')['Hugo_Symbol'].value_counts()\n",
    "    df = df.drop_duplicates(subset=['case_barcode', 'Hugo_Symbol'])\n",
    "    df = df.set_index(['case_barcode', 'Hugo_Symbol'])\n",
    "    df['mutation_count'] = counts\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def reshape_pivot(df_in):\n",
    "    \"\"\"\n",
    "    Reduce df to crucial subset then pivot on cases and genes.\n",
    "    \"\"\"\n",
    "    df = (df_in[['case_barcode', 'Hugo_Symbol', 'mutation_count']]\n",
    "              .copy()\n",
    "              .pivot(index='case_barcode', columns='Hugo_Symbol', values='mutation_count')\n",
    "              .fillna(0)\n",
    "              .astype(int))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_label_df(df_in, df_X):\n",
    "    \"\"\"\n",
    "    Get label df from flat processed df.\n",
    "    \"\"\"\n",
    "    df_y = (df_in.loc[df_in['case_barcode'].isin(df_X.index)]\n",
    "                 .groupby('case_barcode')\n",
    "                 .head(1)\n",
    "                 .set_index('case_barcode')[['project_short_name']]\n",
    "                 .sort_index())\n",
    "    return df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get processed dataframes ready for training\n",
    "df_X1 = (df_proc1.pipe(dedup_and_get_variant_count)\n",
    "                 .pipe(reshape_pivot))\n",
    "\n",
    "df_X2 = (df_proc2.pipe(dedup_and_get_variant_count)\n",
    "                 .pipe(reshape_pivot))\n",
    "\n",
    "df_y1 = (df_proc1.pipe(get_label_df, df_X1))\n",
    "df_y2 = (df_proc2.pipe(get_label_df, df_X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X1.head()\n",
    "df_X2.head()\n",
    "df_y1.head()\n",
    "df_y2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X1.shape\n",
    "df_y1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 6. Produce 2D images for every sample\n",
    "<a id=\"2d\"></a>\n",
    "\n",
    "For each sample, now want to encode gene2vec embeddings as a prior on mutation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X1.shape\n",
    "df_X2.shape\n",
    "\n",
    "def convert_to_onehot(df_in):\n",
    "    \"\"\"\n",
    "    Convert count encoding to one-hot encoded representation of df.\n",
    "    \"\"\"\n",
    "    df = df_in.copy()\n",
    "    df[df != 0] = 1\n",
    "    return df\n",
    "\n",
    "def get_gene_intersection(df1, df2):\n",
    "    \"\"\"\n",
    "    Get intersection of genes (features) from df1 and df2.\n",
    "    \"\"\"\n",
    "    gene_intersection = []\n",
    "    for gene in df1:\n",
    "        if gene in df2.columns.tolist():\n",
    "            gene_intersection.append(gene)\n",
    "\n",
    "    return gene_intersection\n",
    "\n",
    "    \n",
    "def embed_gene_vectors(data_df, gene_df):\n",
    "    \"\"\"\n",
    "    Matrix multiply somatic mutation data by gene embeddings and return batch of images.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for i, row in data_df.iterrows():\n",
    "        # multiply gene embedding by bitwise mask taken from sample mutations\n",
    "        embedded_sample = row.values * gene_df.values\n",
    "        samples.append(embedded_sample)\n",
    "    return np.dstack(samples)\n",
    "\n",
    "def apply_spectral_clustering(data_df, onehot_df, num_clusters=25):\n",
    "    \"\"\"\n",
    "    Determine a sorting on genes which creates visual structure.\n",
    "    Calculates feature cooccurrence matrix, finds num_clusters as defined and sorts genes accordingly.\n",
    "    \"\"\"\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    c_matrix = onehot_df.T.dot(onehot_df) # cooccurrence matrix for genes in data source\n",
    "    sc = SpectralClustering(num_clusters, affinity='precomputed', n_init=100, assign_labels='discretize')\n",
    "    clusters = sc.fit_predict(c_matrix)\n",
    "    return data_df[:,np.argsort(clusters),:]\n",
    "    \n",
    "def visualise_sample(sample):\n",
    "    \"\"\"\n",
    "    Visualise sample as image.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(pd.DataFrame(sample), aspect='auto')\n",
    "    # plt.colorbar()\n",
    "    \n",
    "def visualise_clusters(data_df, onehot_df, index=5000):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialise plot\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize = (18,12))\n",
    "    fig.suptitle('TCGA Sample Number 5000')\n",
    "    \n",
    "    clustered1 = apply_spectral_clustering(data_df[:,:,index:index+1], \n",
    "                                           onehot_df, num_clusters=5)\n",
    "    \n",
    "    clustered2 = apply_spectral_clustering(data_df[:,:,index:index+1], \n",
    "                                           onehot_df, num_clusters=10)\n",
    "    \n",
    "    clustered3 = apply_spectral_clustering(data_df[:,:,index:index+1], \n",
    "                                           onehot_df, num_clusters=20)\n",
    "    \n",
    "    clustered4 = apply_spectral_clustering(data_df[:,:,index:index+1], \n",
    "                                           onehot_df, num_clusters=50)\n",
    "\n",
    "    axes[0,0].imshow(pd.DataFrame(clustered1[:,:,0]), aspect='auto')\n",
    "    axes[0,0].set(title='num_clusters=5')\n",
    "    axes[0,1].imshow(pd.DataFrame(clustered2[:,:,0]), aspect='auto')\n",
    "    axes[0,1].set(title='num_clusters=10')\n",
    "    axes[1,0].imshow(pd.DataFrame(clustered3[:,:,0]), aspect='auto')\n",
    "    axes[1,0].set(title='num_clusters=20')\n",
    "    axes[1,1].imshow(pd.DataFrame(clustered4[:,:,0]), aspect='auto')\n",
    "    axes[1,1].set(title='num_clusters=50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = convert_to_onehot(df_X1)\n",
    "test.values.shape\n",
    "\n",
    "# load test file\n",
    "wv, vocabulary = load_embeddings(filename)\n",
    "df_genevec = pd.DataFrame(wv.transpose(), columns=vocabulary)\n",
    "\n",
    "genes = get_gene_intersection(test, df_genevec)\n",
    "merge1 = test[genes]\n",
    "merge2 = df_genevec[genes]\n",
    "\n",
    "output = embed_gene_vectors(merge1, merge2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freememory()\n",
    "clustered4 = apply_spectral_clustering(output, merge1, num_clusters=20)\n",
    "size(clustered4)\n",
    "visualise_sample(clustered4[:,:,5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_clusters(output, merge1, index=3000)\n",
    "visualise_clusters(output, merge1, index=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image labelling and dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absurdly, necessary image processing functions to create FastAI accepted dataset from 3D Numpy batches of images\n",
    "\n",
    "def create_labels_df(df_y):\n",
    "    \"\"\"    \n",
    "    Create labels df.\n",
    "    \"\"\"\n",
    "    labels_df = (df_y4.copy()\n",
    "                  .reset_index()\n",
    "                  .rename(columns={'case_barcode':'name', 'project_short_name':'label'}))\n",
    "    \n",
    "    return labels_df\n",
    "    \n",
    "\n",
    "def create_labels_csv(labels_df, y_train, y_valid, savedir, img_format='png'):\n",
    "    \"\"\"\n",
    "    Create labels.csv from labels df and knowledge of y_train and y_valid data splits.\n",
    "    \"\"\"\n",
    "    train_labels = labels_df.loc[y_train.index]    \n",
    "    valid_labels = labels_df.loc[y_valid.index]\n",
    "    \n",
    "    def format_name(name, label, split):\n",
    "        return '{}/{}/{}.{}'.format(split, label, name, img_format)\n",
    "    \n",
    "    train_labels['name'] = train_labels.apply(lambda row: format_name(row['name'], row['label'], 'train'), axis=1)\n",
    "    valid_labels['name'] = valid_labels.apply(lambda row: format_name(row['name'], row['label'], 'valid'), axis=1)\n",
    "\n",
    "    pd.concat([train_labels, valid_labels]).to_csv(savedir + 'labels.csv', index=False)\n",
    "\n",
    "\n",
    "def generate_dataset(X, y, data_base='../data/genevec_images/', test_size=0.2):\n",
    "    \"\"\"\n",
    "    Generate image dataset folder structure as per MNIST and other common image datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create labels df from y\n",
    "    labels_df = create_labels_df(y)\n",
    "    y_ = labels_df['label']\n",
    "    \n",
    "    print('Getting train/test/val split:')\n",
    "    # 80/10/10 train test validation\n",
    "    X_train, X_other, y_train, y_other = train_test_split(X, y_, test_size=test_size, \n",
    "                                                          random_state=42, stratify=y_)\n",
    "    \n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_other, y_other, test_size=0.5,\n",
    "                                                        random_state=42, stratify=y_other)\n",
    "    \n",
    "    # Save all data to data directory\n",
    "    data_dir = '{}{}'.format(data_base, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "        \n",
    "    # Create labels.csv from y_train, y_valid\n",
    "    create_labels_csv(labels_df, y_train, y_valid, savedir=data_dir + '/')\n",
    "    \n",
    "    dirs = ('train', 'valid', 'test')\n",
    "    ys = (y_train, y_valid, y_test)\n",
    "    Xs = (X_train, X_valid, X_test)\n",
    "\n",
    "    for i, dir_ in enumerate(dirs):\n",
    "        path = data_dir + '/' + dir_\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        if (dir_ == 'test'):\n",
    "            for j, row in enumerate(ys[i]):\n",
    "                image_name = '{}.png'.format(labels_df.loc[ys[i].index[j]]['name'])\n",
    "                img_path = path + '/' + image_name\n",
    "                img.imsave(img_path, Xs[i][j,:,:])\n",
    "        else:\n",
    "            for label in set(ys[i].values):\n",
    "                path_ = path + '/' + label\n",
    "                if not os.path.exists(path_):\n",
    "                    os.mkdir(path_)\n",
    "                for j, row in enumerate(ys[i]):\n",
    "                    if row == label:\n",
    "                        image_name = '{}.png'.format(labels_df.loc[ys[i].index[j]]['name'])\n",
    "                        img_path = path_ + '/' + image_name\n",
    "                        img.imsave(img_path, Xs[i][j,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df = apply_spectral_clustering(output, merge1, num_clusters=20)\n",
    "\n",
    "df_X4 = clustered_df.transpose()\n",
    "df_y4 = df_y1\n",
    "\n",
    "df_X4.shape\n",
    "df_y4.shape\n",
    "\n",
    "generate_dataset(df_X4, df_y4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 7. Model Training\n",
    "<a id=\"training\"></a>\n",
    "\n",
    "##### 7.1 Ensemble of sklearn models on flat data (baseline on sparse samples)\n",
    "<a id=\"sklearn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "            \n",
    "    title = 'Confusion Matrices'\n",
    "\n",
    "    # Compute confusion matrices\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            \n",
    "    # Initialise plot\n",
    "    fig, axes = plt.subplots(ncols=2, figsize = (20,11))\n",
    "    fig.suptitle(title)\n",
    "    plt.subplots_adjust(hspace=0.6)\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i == 1:\n",
    "            normalise = True\n",
    "            data = cm_norm\n",
    "            title = 'Normalised'\n",
    "        else:\n",
    "            normalise = False\n",
    "            data = cm\n",
    "            title = 'Non-Normalised'\n",
    "            \n",
    "        ax.imshow(data, interpolation='nearest')\n",
    "        \n",
    "#         ax.figure.colorbar()\n",
    "        # We want to show all ticks...\n",
    "        ax.set(xticks=np.arange(data.shape[1]),\n",
    "               yticks=np.arange(data.shape[0]),\n",
    "               # ... and label them with the respective list entries\n",
    "               xticklabels=classes, yticklabels=classes,\n",
    "               title=title,\n",
    "               ylabel='True label',\n",
    "               xlabel='Predicted label')\n",
    "\n",
    "        # Rotate the tick labels and set their alignment.\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "        # Loop over data dimensions and create text annotations.\n",
    "        fmt = '.1f' if normalise else 'd'\n",
    "        thresh = data.max() / 2.\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(data.shape[1]):\n",
    "                ax.text(j, i, format(data[i, j], fmt),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if data[i, j] < thresh else \"black\")\n",
    "            \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function that runs the requested algorithm and returns the accuracy metrics\n",
    "def fit_ml_algo(algo, X_train, y_train, cv):\n",
    "    # CV \n",
    "    train_pred = model_selection.cross_val_predict(algo, \n",
    "                                                  X_train, \n",
    "                                                  y_train, \n",
    "                                                  cv=cv, \n",
    "                                                  n_jobs = -1)\n",
    "    \n",
    "    cv_proba = model_selection.cross_val_predict(algo, \n",
    "                                                  X_train, \n",
    "                                                  y_train, \n",
    "                                                  cv=cv, \n",
    "                                                  method='predict_proba',\n",
    "                                                  n_jobs=-1)\n",
    "    \n",
    "    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n",
    "    return train_pred, acc_cv, cv_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Trees\n",
    "def run_model(X_train, y_train, model, model_name, data_name, cv=10):\n",
    "    start_time = time.time()\n",
    "    train_pred, acc_cv, cv_proba = fit_ml_algo(model, \n",
    "                                             X_train, \n",
    "                                             y_train, \n",
    "                                             cv)\n",
    "    gbt_time = (time.time() - start_time)\n",
    "    \n",
    "    print(\"Model: %s, DataFrame: %s\" % (model_name, data_name))\n",
    "    print(\"Accuracy CV 10-Fold: %s\" % acc_cv)\n",
    "    print(\"Running Time: %s\" % datetime.timedelta(seconds=gbt_time))\n",
    "    \n",
    "    print(metrics.classification_report(y_train, train_pred))\n",
    "    \n",
    "    save_dir = EXP1_DIR + \"{}_{}_\".format(data_name, model_name)\n",
    "    \n",
    "    # save model\n",
    "    with open(save_dir + \"model.pkl\", 'wb') as outfile:\n",
    "        pkl.dump(model, outfile)\n",
    "    \n",
    "    # save scores\n",
    "    with open(save_dir + \"scores.pkl\", 'wb') as outfile:\n",
    "        pkl.dump((train_pred, acc_cv, cv_proba), outfile)\n",
    "    \n",
    "    return train_pred, acc_cv, cv_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models2 = {\n",
    "        'log' : {\n",
    "            'model' : LogisticRegression(n_jobs = -1),\n",
    "        },\n",
    "        \n",
    "        'knn' : {\n",
    "            'model' : KNeighborsClassifier(n_neighbors = 3,\n",
    "                                           n_jobs = -1)\n",
    "        },\n",
    "    \n",
    "        'gaussian' : {\n",
    "            'model' : GaussianNB()   \n",
    "        },\n",
    "    \n",
    "        'sgd' : {\n",
    "            'model' : CalibratedClassifierCV(base_estimator=SGDClassifier(loss='hinge', \n",
    "                                                                          max_iter=1, \n",
    "                                                                          n_jobs = -1), \n",
    "                                             method='sigmoid', \n",
    "                                             cv=10)\n",
    "        },\n",
    "    \n",
    "        'dt' : {\n",
    "            'model' : DecisionTreeClassifier()\n",
    "        },\n",
    "    \n",
    "        'rf' : {\n",
    "            'model' : RandomForestClassifier(n_estimators=10, \n",
    "                                             min_samples_leaf=2,\n",
    "                                             min_samples_split=17, \n",
    "                                             criterion='gini', \n",
    "                                             max_features=8)\n",
    "        }\n",
    "    \n",
    "#         'gbt' : { \n",
    "#             'model' : GradientBoostingClassifier(n_estimators=20, \n",
    "#                                                  min_samples_leaf=2,\n",
    "#                                                  min_samples_split=17, \n",
    "#                                                  max_features=20)\n",
    "#         }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models2.items():\n",
    "    model['train_pred'], model['acc_cv'], model['cv_proba'] = run_model(df_X1, df_y1, model['model'], name, 'df_1')\n",
    "    plot_confusion_matrix(df_y1, \n",
    "                          model['train_pred'], \n",
    "                          classes=df_y1['project_short_name'].value_counts().index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {'model_name':[], 'acc_cv_%':[]}\n",
    "for name, model in models2.items():\n",
    "    results_dict['model_name'].append(name)\n",
    "    results_dict['acc_cv_%'].append(model['acc_cv'])\n",
    "    \n",
    "(pd.DataFrame(results_dict)\n",
    "    .sort_values('acc_cv_%', ascending=False)\n",
    "    .set_index('model_name')\n",
    "    .plot.bar(figsize = (10,6), title='model accuracy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 7.2 Random Forest on images\n",
    "<a id=\"rf\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models3 = {\n",
    "        'dt' : {\n",
    "            'model' : DecisionTreeClassifier()\n",
    "        },\n",
    "    \n",
    "        'rf' : {\n",
    "            'model' : RandomForestClassifier(n_estimators=10, \n",
    "                                             min_samples_leaf=2,\n",
    "                                             min_samples_split=17, \n",
    "                                             criterion='gini', \n",
    "                                             max_features=8)\n",
    "        },\n",
    "    \n",
    "        'gbt' : { \n",
    "            'model' : GradientBoostingClassifier(n_estimators=20, \n",
    "                                                 min_samples_leaf=2,\n",
    "                                                 min_samples_split=17, \n",
    "                                                 max_features=20)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten images to sparse feature vectors\n",
    "img_flattened = output.reshape(-1, output.shape[-1])\n",
    "img_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X3 = img_flattened.transpose()\n",
    "df_y3 = df_y1\n",
    "\n",
    "for name, model in models3.items():\n",
    "    model['train_pred'], model['acc_cv'], model['cv_proba'] = run_model(df_X3, df_y3, model['model'], name, 'df_3')\n",
    "    plot_confusion_matrix(df_y1, \n",
    "                          model['train_pred'], \n",
    "                          classes=df_y1['project_short_name'].value_counts().index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.3 ResNet50 w ImageNet weight initialisation (FastAI)\n",
    "<a id=\"fastai\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ImageList.from_folder('../data/genevec_images/20190522-075618/')\n",
    "        .split_by_folder(train='train', valid='valid')          \n",
    "        .label_from_folder()\n",
    "        .transform(size=512)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(rows=3, figsize=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freememory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freegpu(data, model):\n",
    "    del data\n",
    "    del model\n",
    "    freememory()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "freegpu(data, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To create a ResNET 50 with pretrained weights\n",
    "learn = cnn_learner(data, models.resnet50, metrics=error_rate, callback_fns=ShowGraph)\n",
    "learn.fit_one_cycle(10)\n",
    "learn.save('stage-1-256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(20,20), dpi=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, max_lr=slice(1e-4,1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(20,20), dpi=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, max_lr=slice(1e-5,1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary:\n",
    "    if '\\n' in word:\n",
    "        print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis] *",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
