{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Unsupervised Language Model\n",
    "\n",
    "1. Initialisation\n",
    "2. Training\n",
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from utils import generate_fixed_vocab, generate_variable_vocab, get_model_LM\n",
    "import sys; sys.path.append(\"../tools\"); from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter = pd.read_csv(HUMAN/'human_genome_data_fa.csv', chunksize=NROWS_TRAIN+NROWS_VAL)\n",
    "df = next(df_iter)\n",
    "\n",
    "# set val to be first 20k rows\n",
    "df_tr = df[:NROWS_TRAIN]\n",
    "df_va = df[NROWS_TRAIN:NROWS_TRAIN+NROWS_VAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Experiment 1: Fixed Length Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = dict(emb_sz=400, \n",
    "                    n_hid=1150, \n",
    "                    n_layers=3, \n",
    "                    pad_token=0, \n",
    "                    qrnn=False, \n",
    "                    output_p=0.25, \n",
    "                    hidden_p=0.1, \n",
    "                    input_p=0.2, \n",
    "                    embed_p=0.02, \n",
    "                    weight_p=0.15, \n",
    "                    tie_weights=True, \n",
    "                    out_bias=True)\n",
    "\n",
    "DROP_MULT   = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = []\n",
    "\n",
    "# fixed length\n",
    "for ngram_stride in NGRAM_STRIDE:\n",
    "    experiment = {}\n",
    "    experiment['title'] = 'fixed_{}_{}'.format(*ngram_stride)\n",
    "    experiment['xdata'], experiment['vocab'] = generate_fixed_vocab(df_tr, df_va, *ngram_stride)\n",
    "    \n",
    "    experiments.append(experiment)\n",
    "\n",
    "# variable length   \n",
    "for max_vocab in MAX_VOCAB:\n",
    "    experiment = {}\n",
    "    experiment['title'] = 'variable_{}'.format(max_vocab)    \n",
    "    experiment['xdata'], experiment['vocab'] = generate_variable_vocab(df_tr, df_va, max_vocab)\n",
    "    \n",
    "    experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed_3_1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tie_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e530b327a4bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-e530b327a4bf>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(experiment, n_cycles, lr_find)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdata\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xdata'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_LM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_fp16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;31m# convert model weights to 16-bit float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/notebooks/2_seq_modelling/utils.py\u001b[0m in \u001b[0;36mget_model_LM\u001b[0;34m(data, drop_mult, config, wd)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mdrop_mult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mtie_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'tie_weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output_p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'out_bias'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAWD_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tie_weights'"
     ]
    }
   ],
   "source": [
    "def train_model(experiment, n_cycles=1, lr_find=False):\n",
    "    \n",
    "    config      = MODEL_CONFIG\n",
    "    drop_mult   = DROP_MULT\n",
    "    \n",
    "    data  = experiment['xdata']\n",
    "    learn = get_model_LM(data, drop_mult, config)\n",
    "    learn = learn.to_fp16(dynamic=True); # convert model weights to 16-bit float\n",
    "    \n",
    "    # add callbacks\n",
    "    from fastai.callbacks.csv_logger import CSVLogger\n",
    "    learn.callback_fns.append(partial(CSVLogger, append=True))\n",
    "    \n",
    "    if lr_find:\n",
    "        learn.lr_find()\n",
    "        learn.recorder.plot()\n",
    "    \n",
    "    learn.fit_one_cycle(n_cycles, 2e-2, moms=(0.8, 0.7))\n",
    "                \n",
    "    learn.save(experiment['title'])\n",
    "    learn.save_encoder(experiment['title'])\n",
    "    \n",
    "    # free up cuda\n",
    "    del learn; del data; torch.cuda.empty_cache()\n",
    "\n",
    "for experiment in experiments:\n",
    "    print(experiment['title'])\n",
    "    train_model(experiment, n_cycles=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freememory():\n",
    "    \"\"\"\n",
    "    Run garbage collection to free up memory.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "freememory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboardX\n",
    "from fastai.callbacks.tensorboard import LearnerTensorboardWriter\n",
    "from pathlib import Path\n",
    "\n",
    "project_id = 'exp1'\n",
    "tboard_path = Path('./logs/' + project_id)\n",
    "learn.callback_fns.append(partial(LearnerTensorboardWriter, \n",
    "                                    base_dir=tboard_path, \n",
    "                                    name='run1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GenomicTextLMDataBunch.from_df(path, df[20000:40000], df_val, bs=3000, tokenizer=tok, vocab=model_vocab, max_vocab=80000,\n",
    "                              chunksize=10000, text_cols=0, label_cols=1)\n",
    "\n",
    "config = dict(emb_sz=400, \n",
    "              n_hid=1150, \n",
    "              n_layers=3, \n",
    "              pad_token=0, \n",
    "              qrnn=False, \n",
    "              output_p=0.25, \n",
    "              hidden_p=0.1, \n",
    "              input_p=0.2, \n",
    "              embed_p=0.02, \n",
    "              weight_p=0.15, \n",
    "              tie_weights=True, \n",
    "              out_bias=True)\n",
    "\n",
    "drop_mult=0.3\n",
    "\n",
    "learn = get_model_LM(data, drop_mult, config)\n",
    "learn = learn.to_fp16(dynamic=True);\n",
    "\n",
    "learn.fit_one_cycle(2, 5e-3, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = np.load(path/'human_vocab_3m1s.npy')\n",
    "model_vocab = GenomicVocab(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "lr = 5e-3\n",
    "for df in df_iter:\n",
    "    data = GenomicTextLMDataBunch.from_df(path, df, df_val, bs=800, tokenizer=tok, vocab=model_vocab, max_vocab=80000,\n",
    "                                  chunksize=20000, text_cols=0, label_cols=1)\n",
    "    learn.data = data                        \n",
    "    lr_iter = lr/1.5**count\n",
    "    print(f'Learning Rate: {lr_iter}')\n",
    "    learn.fit_one_cycle(1, lr, moms=(0.8,0.7))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('human_3m1s2')\n",
    "learn.save_encoder('human_3m1s_enc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('human_3m1s2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.to_fp32();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('human_3m1s2_fp32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-thesis] *",
   "language": "python",
   "name": "conda-env-.conda-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
