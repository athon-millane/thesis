{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Unsupervised Language Model\n",
    "\n",
    "- 2.1 Initialisation\n",
    "- 2.2 Training\n",
    "- 2.3 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from utils import tok_fixed, tok_variable, get_model_LM\n",
    "import sys; sys.path.append(\"../tools\"); from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter = pd.read_csv(HUMAN/'human_genome_data_fa.csv', chunksize=NROWS_TRAIN+NROWS_VAL)\n",
    "df = next(df_iter)\n",
    "\n",
    "# set val to be first 20k rows\n",
    "df_tr = df[:NROWS_TRAIN]\n",
    "df_va = df[NROWS_TRAIN:NROWS_TRAIN+NROWS_VAL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = dict(emb_sz=400, \n",
    "                    n_hid=1150, \n",
    "                    n_layers=3, \n",
    "                    pad_token=0, \n",
    "                    qrnn=False, \n",
    "                    output_p=0.25, \n",
    "                    hidden_p=0.1, \n",
    "                    input_p=0.2, \n",
    "                    embed_p=0.02, \n",
    "                    weight_p=0.15, \n",
    "                    tie_weights=True, \n",
    "                    out_bias=True)\n",
    "\n",
    "DROP_MULT   = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the required batch size to fit on CUDA, we can use the inverse proportionality of batch size with number of features, which is 4^n_gram. We know that a batch of 4^3 features will need to be 4^3/4^2 = 4 times smaller than for 4^2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = []\n",
    "\n",
    "# fixed length\n",
    "for i,ngram_stride in enumerate(NGRAM_STRIDE):\n",
    "    experiment = {}\n",
    "    experiment['title'] = 'fixed_{}_{}_rows_{}'.format(*ngram_stride,NROWS_TRAIN)\n",
    "    experiment['xdata'], experiment['vocab'] = tok_fixed(df_tr, df_va, *ngram_stride, bs=BS[i])\n",
    "    \n",
    "    experiments.append(experiment)\n",
    "\n",
    "# variable length   \n",
    "for i,max_vocab in enumerate(MAX_VOCAB):\n",
    "    experiment = {}\n",
    "    experiment['title'] = 'variable_{}_rows_{}'.format(max_vocab,NROWS_TRAIN)    \n",
    "    experiment['xdata'], experiment['vocab'] = tok_variable(df_tr, df_va, max_vocab, bs=BS[i])\n",
    "    \n",
    "    experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Language Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(experiment, epochs=10):\n",
    "    import torch; import os\n",
    "    config      = MODEL_CONFIG.copy()\n",
    "    drop_mult   = DROP_MULT\n",
    "    \n",
    "    data  = experiment['xdata']\n",
    "    learn = get_model_LM(data, drop_mult, config)\n",
    "    learn = learn.to_fp16(dynamic=True); # convert model weights to 16-bit float\n",
    "    \n",
    "    model = 'models/' + experiment['title'] + '.pth'\n",
    "    if os.path.exists(HUMAN/model):\n",
    "        print('model found: loading model: {}'.format(experiment['title']))\n",
    "        learn.load(experiment['title'])\n",
    "        learn.data = data\n",
    "\n",
    "    # add callbacks\n",
    "    from fastai.callbacks.csv_logger import CSVLogger\n",
    "    learn.callback_fns.append(partial(CSVLogger, \n",
    "                                      filename='history_' + experiment['title'], \n",
    "                                      append=True))\n",
    "    \n",
    "    learn.fit_one_cycle(epochs, 5e-3, moms=(0.8, 0.7))\n",
    "    learn.save(experiment['title'])\n",
    "    learn.save_encoder(experiment['title']+'_enc')\n",
    "    \n",
    "    # free up cuda\n",
    "    del learn; del data; torch.cuda.empty_cache()\n",
    "\n",
    "for experiment in experiments[2:]:\n",
    "    print(experiment['title'])\n",
    "    train_model(experiment, epochs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment['xdata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(learn):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig,ax = plt.subplots(2,1,figsize=(8,12))\n",
    "    ax[0].plot(list(range(len(learn.recorder.val_losses))),learn.recorder.val_losses, label='Validation loss')\n",
    "    ax[0].plot(list(range(len(learn.recorder.val_losses))),\n",
    "               [learn.recorder.losses[i] for i in range(len(learn.recorder.val_losses),\n",
    "                                                        len(learn.recorder.losses),\n",
    "                                                        len(learn.recorder.losses)//len(learn.recorder.val_losses))], \n",
    "               label='Training loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend(loc='upper right')\n",
    "    ax[1].plot(list(range(len(learn.recorder.val_losses))),learn.recorder.metrics)\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "\n",
    "def train_sequence(experiment):\n",
    "    config      = MODEL_CONFIG.copy()\n",
    "    drop_mult   = DROP_MULT\n",
    "    data,vocab  = generate_variable_vocab(df_tr, df_va, 64)\n",
    "    learn = get_model_LM(data, drop_mult, config)\n",
    "    learn = learn.to_fp16(dynamic=True); # convert model weights to 16-bit float\n",
    "    \n",
    "    count = 0\n",
    "    lr = 5e-3\n",
    "    for df in df_iter:\n",
    "        data,_ = generate_variable_vocab(df, df_va, 64)\n",
    "        learn.data = data                        \n",
    "        lr_iter = lr/1.5**count\n",
    "        print(f'Learning Rate: {lr_iter}')\n",
    "        learn.fit_one_cycle(1, lr_iter, moms=(0.8,0.7))\n",
    "        count += 1\n",
    "        \n",
    "        plot_losses(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorboardX\n",
    "# from fastai.callbacks.tensorboard import LearnerTensorboardWriter\n",
    "# from pathlib import Path\n",
    "\n",
    "# project_id = 'exp1'\n",
    "# tboard_path = Path('./logs/' + project_id)\n",
    "# learn.callback_fns.append(partial(LearnerTensorboardWriter, \n",
    "#                                     base_dir=tboard_path, \n",
    "#                                     name='run1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-thesis] *",
   "language": "python",
   "name": "conda-env-.conda-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
