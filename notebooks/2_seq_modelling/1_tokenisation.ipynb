{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Genome Language Model\n",
    "1. Initialisation\n",
    "    - 1.1 Imports\n",
    "    - 1.2 Data processing\n",
    "2. Tokenisation\n",
    "    - 2.1 Fixed Length Tokenisation\n",
    "    - 2.2 Variable Length Tokenisation\n",
    "3. Experiments\n",
    "    - 3.1 Comparing fixed and variable length tokenisation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Initialisation\n",
    "## 1.1 Imports and config\n",
    "Libraries used include:\n",
    "- `fastai` for access to the ULMFiT model API\n",
    "- `pandas` and `matplotlib` for data science toolset\n",
    "\n",
    "Handy tricks include:\n",
    "- `autoreload` functionality so that updates to library code are automatically recompiled any time a cell is run.\n",
    "- `InteractiveShell` is used for multiple outputs from a single cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "import sentencepiece as spm\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import cupy as np\n",
    "\n",
    "# Imports from GenomicULMFiT repo\n",
    "sys.path.append(\"../../../Genomic-ULMFiT/\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from src.processing import process_fasta\n",
    "from src.config import GRCH38_P13, GENOME\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN = Path('/home/jovyan/ml_genomics/genomeXL/data/human/')\n",
    "THESIS = Path('/home/jovyan/ml_genomics/thesis/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset params\n",
    "NROWS_TRAIN     = 1000\n",
    "NROWS_VAL       = 1000\n",
    "\n",
    "# Tokenisation - fixed\n",
    "NGRAM_STRIDE    = [(3,1),(5,3),(7,5)]  #(ngram,stride) combinations for tokenisation\n",
    "\n",
    "# Tokenisation - variable\n",
    "MAX_VOCAB       = [64, 1024, 16438]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Initialisation\n",
    "This process assumes data has already been read and processed into `.csv` format via the `process_fasta` script.\n",
    "Data is read from `human_genome_data_fa.csv` in chunks to reduce data in memory. This can then be iterated with `df.next()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter = pd.read_csv(HUMAN/'human_genome_data_fa.csv', chunksize=NROWS_TRAIN+NROWS_VAL)\n",
    "df = next(df_iter)\n",
    "\n",
    "# set val to be first 20k rows\n",
    "df_tr = df[:NROWS_TRAIN]\n",
    "df_va = df[NROWS_TRAIN:NROWS_TRAIN+NROWS_VAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Tokenisation\n",
    "## 2.1 Fixed length tokenisation\n",
    "- Technique of original author was to iterate over entire genome sequence with for loops.\n",
    "- We need to vectorise this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tools.tokenizers import seq_tokenizer, vec_tokenizer\n",
    "\n",
    "def seq_tokenizer(t, ngram, stride):\n",
    "    import time\n",
    "    t = t.upper()\n",
    "    if ngram == 1:\n",
    "        toks = list(t)\n",
    "    else:\n",
    "        start = time.time()\n",
    "        toks = [t[i:i+ngram] for i in range(0, len(t), stride) if len(t[i:i+ngram]) == ngram]\n",
    "\n",
    "    if len(toks[-1]) < ngram:\n",
    "        toks = toks[:-1]\n",
    "\n",
    "    return toks,time.time() - start\n",
    "\n",
    "def vec_tokenizer(a, ngram=3, stride=1, padnum=0):\n",
    "    import time\n",
    "    start = time.time()\n",
    "    a = np.array(list(a)).astype(object)\n",
    "    n = a.strides[0]\n",
    "    nrows = ((a.size)//stride)\n",
    "    strided = np.lib.stride_tricks.as_strided\n",
    "    out = strided(a[(ngram-1):-(stride-1)], shape=(nrows,ngram), strides=(stride*n,-n))[:,::-1] \n",
    "    out = out[ngram-1:,:]\n",
    "    toks = list(out[:,0] + out[:,1] + out[:,2])\n",
    "    return toks,time.time() - start\n",
    "\n",
    "# Test how well the vectorisation works\n",
    "def test_tokenizers():\n",
    "    seq_=[]; vec_=[]\n",
    "    for i in [1,10,100,1000,10000]:\n",
    "        test = ''.join([''.join(row) for row in df.head(i).values[:,0]])\n",
    "        toks ,time = seq_tokenizer(test, 3, 1);seq_.append(time)\n",
    "        toks_,time_ = vec_tokenizer(test, 3, 1);vec_.append(time_)\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "    stats=pd.DataFrame({'seq':seq_,'vec':vec_})\n",
    "    stats.plot(ax=ax)\n",
    "    ax.set_title('Sequential vs Vectorised Tokenisation')\n",
    "    ax.set_ylabel('Time')\n",
    "    \n",
    "test_tokenizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorised technique for faster fixed Length tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class FixedLengthGenomicTokenizer(BaseTokenizer):\n",
    "    \"\"\"\n",
    "    Fixed length tokenisation for DNA.\n",
    "    \"\"\"\n",
    "    def __init__(self, lang='en', ngram=5, stride=2):\n",
    "        self.lang = lang\n",
    "        self.ngram = ngram\n",
    "        self.stride = stride\n",
    "        \n",
    "    def tokenizer(self, t):\n",
    "        t = t.upper()\n",
    "        if self.ngram == 1:\n",
    "            toks = list(t)\n",
    "        else:\n",
    "            toks = [t[i:i+self.ngram] for i in range(0, len(t), self.stride) if len(t[i:i+self.ngram]) == self.ngram]\n",
    "        if len(toks[-1]) < self.ngram:\n",
    "            toks = toks[:-1]\n",
    "        return toks\n",
    "    \n",
    "    def add_special_cases(self, toks):\n",
    "        pass\n",
    "    \n",
    "def generate_fixed_vocab(df_train, df_val, ngram=3, stride=1):\n",
    "    \"\"\"Create fixed length tokenizer, initialise databunch and return vocabulary.\"\"\"\n",
    "    \n",
    "    # initialise tokeniser\n",
    "    tok = Tokenizer(partial(GenomicTokenizer, ngram=ngram, stride=stride), \n",
    "                    n_cpus=40,\n",
    "                    pre_rules=[],\n",
    "                    post_rules=[],\n",
    "                    special_cases=[])\n",
    "    \n",
    "    data = GenomicTextLMDataBunch.from_df(HUMAN, df_train, df_val, bs=NROWS_TRAIN, tokenizer=tok, \n",
    "                              chunksize=NROWS_TRAIN, text_cols=0, label_cols=1, max_vocab=((4**ngram)+1))\n",
    "\n",
    "    # Save and load vocab\n",
    "    np.save(HUMAN / 'fixed_vocab_{}m{}s.npy'.format(ngram,stride), data.vocab.itos)\n",
    "    return data.vocab.itos\n",
    "    \n",
    "vocabs = []\n",
    "for ngram_stride in NGRAM_STRIDE:\n",
    "    vocabs.append(generate_fixed_vocab(df_tr, df_va, *ngram_stride))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_variable_vocab(df_train, df_val, size=128):\n",
    "    \"\"\"Create variable length vocabulary using SentencePiece tokenisation.\n",
    "    \"\"\"\n",
    "    sp_proc = SPProcessor(char_coverage=1, \n",
    "                          vocab_sz=size,\n",
    "                          n_cpus = 40)\n",
    "    \n",
    "    data = GenomicTextLMDataBunch.from_df(\n",
    "        HUMAN, df_train, df_val, bs=NROWS_TRAIN, processor=sp_proc,\n",
    "        chunksize=NROWS_TRAIN, text_cols=0, label_cols=1, max_vocab=size\n",
    "    )\n",
    "    \n",
    "    # Save and load vocab\n",
    "    np.save(HUMAN / 'variable_vocab_{}tok.npy'.format(size), data.vocab.itos)\n",
    "    return data.vocab.itos\n",
    "    \n",
    "for max_vocab in MAX_VOCAB:\n",
    "    vocabs.append(generate_variable_vocab(df_tr, df_va, size=max_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(vocabs).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable length tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df) * len(df.values[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_MODEL = str(THESIS / \"models/genome10M/spiece.model\")\n",
    "\n",
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(str(THESIS / \"models/genome10M/spiece.model\"))\n",
    "\n",
    "# encode: text => id\n",
    "print(sp.encode_as_pieces('GTGAAGGAGCAGGGGCTCCACGTCTGGCGACAACCAGGGAA'))\n",
    "print(sp.encode_as_ids('GTGAAGGAGCAGGGGCTCCACGTCTGGCGACAACCAGGGAA'))\n",
    "\n",
    "# decode: id => text\n",
    "print(sp.decode_pieces(['‚ñÅ', 'GTGA', 'AGGAG', 'CAGG', 'GGCT', 'CCAC', 'GTC', 'TGGC', 'GACA', 'ACC', 'AGGGAA']))\n",
    "print(sp.decode_ids([45, 249, 203, 57, 199, 87, 72, 181, 251, 102, 520]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tok_dist():\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=2, figsize=(12,8))\n",
    "    \n",
    "    pd.Series(tokens)\\\n",
    "        .value_counts().head(20).plot.bar(ax=axs[0])\n",
    "    axs[0].set_title('Most common tokens')\n",
    "    axs[0].set_ylabel('Count')\n",
    "    \n",
    "    pd.Series([len(token) for token in tokens])\\\n",
    "        .value_counts().sort_index().plot.bar(ax=axs[1])\n",
    "    axs[1].set_title('Token Length Distribution')\n",
    "    axs[1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "plot_tok_dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Setup\n",
    "We propose to explore model performance across different fixed and variable token parameters.\n",
    "For fixed length tokenisation, the parameters of interest are the number of base pairs per token, also known as `ngram`.\n",
    "Additionally, the number of base pairs between the start of 1 token and the next is known as `stride`. As an example, take the sequence of base pairs:\n",
    "$$\\text{TCTGGCGACAACCAGGGA}$$\n",
    "\n",
    "Using fixed length tokenisations of size **_3_** and stride **_0_**, we have the following outputs:\n",
    "$$\\text{[TCT],[GGC],[GAC],[AAC],[CAG],[GGA]}$$\n",
    "\n",
    "For parameters: `{size:3, stride:1}`\n",
    "$$\\text{[TCT],[],[],[GGC],[],[],[GAC],[],[],[AAC],[],[],[CAG],[],[],[GGA]}$$\n",
    "\n",
    "For parameters: `{size:5, stride:3}`\n",
    "$$\\text{[TCT],[GGC],[GAC],[AAC],[CAG],[GGA]}$$\n",
    "\n",
    "For parameters: `{size:7, stride:5}`\n",
    "$$\\text{[TCT],[GGC],[GAC],[AAC],[CAG],[GGA]}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # initialise tokeniser\n",
    "tok = Tokenizer(partial(GenomicTokenizer, ngram=ngram, stride=1), \n",
    "                n_cpus=64, \n",
    "                pre_rules=[], \n",
    "                post_rules=[], \n",
    "                special_cases=['xxpad'])\n",
    "\n",
    "# Fixed length tokenisation - 3m1\n",
    "data_fixed = GenomicTextLMDataBunch.from_df(\n",
    "    HUMAN, df[20000:40000], df_val, tokenizer=tok,\n",
    "    bs=3200, vocab=model_vocab, max_vocab=len(voc),\n",
    "    chunksize=10000, text_cols=0, label_cols=1)\n",
    "\n",
    "# SentencePiece Tokenisation\n",
    "sp_proc = SPProcessor(char_coverage=1, \n",
    "                      vocab_sz=size,\n",
    "                      n_cpus = 40)\n",
    "\n",
    "data_var = data = GenomicTextLMDataBunch.from_df(\n",
    "    HUMAN, df[20000:], df_val, bs=800, processor=sp_proc, \n",
    "    chunksize=10000, text_cols=0, label_cols=1, max_vocab=4^ngram+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocab\n",
    "ngram       = 3\n",
    "stride      = 1\n",
    "voc         = np.load(HUMAN / 'human_vocab_3m1s.npy')\n",
    "model_vocab = GenomicVocab(voc)\n",
    "\n",
    "print(\"Loaded vocabulary. Number of tokens: {}. Mean token length: {}.\" \\\n",
    "      .format(len(voc), len(voc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocab\n",
    "ngram       = 3\n",
    "stride      = 1\n",
    "voc         = np.load(HUMAN / 'human_vocab_3m1s.npy')\n",
    "model_vocab = GenomicVocab(voc)\n",
    "\n",
    "print(\"Loaded vocabulary. Number of tokens: {}. Mean token length: {}.\".format(len(voc), len(voc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # initialise tokeniser\n",
    "tok = Tokenizer(partial(GenomicTokenizer, ngram=ngram, stride=1), \n",
    "                n_cpus=64, \n",
    "                pre_rules=[], \n",
    "                post_rules=[], \n",
    "                special_cases=['xxpad'])\n",
    "\n",
    "data = GenomicTextLMDataBunch.from_df(\n",
    "    HUMAN, df[20000:40000], df_val, tokenizer=tok,\n",
    "    bs=3200, vocab=model_vocab, max_vocab=len(voc),\n",
    "    chunksize=10000, text_cols=0, label_cols=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(emb_sz=400, \n",
    "              n_hid=1150, \n",
    "              n_layers=3, \n",
    "              pad_token=0, \n",
    "              qrnn=False, \n",
    "              output_p=0.25, \n",
    "              hidden_p=0.1, \n",
    "              input_p=0.2, \n",
    "              embed_p=0.02, \n",
    "              weight_p=0.15, \n",
    "              tie_weights=True, \n",
    "              out_bias=True)\n",
    "\n",
    "drop_mult=0.3\n",
    "\n",
    "learn = get_model_LM(data, drop_mult, config)\n",
    "learn = learn.to_fp16(dynamic=True); # convert model weights to 16-bit float\n",
    "\n",
    "learn.load('human_3m1s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, 5e-3, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('human_3m1s')\n",
    "learn.save_encoder('human_3m1s_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks.tensorboard import LearnerTensorboardWriter\n",
    "from fastai.callbacks.csv_logger import CSVLogger\n",
    "from pathlib import Path\n",
    "\n",
    "project_id = 'exp1'\n",
    "tboard_path = Path('./logs/' + project_id)\n",
    "learn.callback_fns.append(partial(LearnerTensorboardWriter, \n",
    "                                    base_dir=tboard_path, \n",
    "                                    name='run1'))\n",
    "\n",
    "learn.callback_fns.append(partial(CSVLogger, append=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 5e-3, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GenomicTextLMDataBunch.from_df(path, df[20000:40000], df_val, bs=3000, tokenizer=tok, vocab=model_vocab, max_vocab=80000,\n",
    "                              chunksize=10000, text_cols=0, label_cols=1)\n",
    "\n",
    "config = dict(emb_sz=400, \n",
    "              n_hid=1150, \n",
    "              n_layers=3, \n",
    "              pad_token=0, \n",
    "              qrnn=False, \n",
    "              output_p=0.25, \n",
    "              hidden_p=0.1, \n",
    "              input_p=0.2, \n",
    "              embed_p=0.02, \n",
    "              weight_p=0.15, \n",
    "              tie_weights=True, \n",
    "              out_bias=True)\n",
    "\n",
    "drop_mult=0.3\n",
    "\n",
    "learn = get_model_LM(data, drop_mult, config)\n",
    "learn = learn.to_fp16(dynamic=True);\n",
    "\n",
    "learn.fit_one_cycle(2, 5e-3, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = np.load(path/'human_vocab_3m1s.npy')\n",
    "model_vocab = GenomicVocab(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "lr = 5e-3\n",
    "for df in df_iter:\n",
    "    data = GenomicTextLMDataBunch.from_df(path, df, df_val, bs=800, tokenizer=tok, vocab=model_vocab, max_vocab=80000,\n",
    "                                  chunksize=20000, text_cols=0, label_cols=1)\n",
    "    learn.data = data                        \n",
    "    lr_iter = lr/1.5**count\n",
    "    print(f'Learning Rate: {lr_iter}')\n",
    "    learn.fit_one_cycle(1, lr, moms=(0.8,0.7))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('human_3m1s2')\n",
    "learn.save_encoder('human_3m1s_enc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('human_3m1s2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.to_fp32();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('human_3m1s2_fp32')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
