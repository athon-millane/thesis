{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Genome Language Model\n",
    "1. Initialisation\n",
    "    - 1.1 Imports\n",
    "    - 1.2 Data processing\n",
    "2. Tokenisation\n",
    "    - 2.1 Fixed Length Tokenisation\n",
    "    - 2.2 Variable Length Tokenisation\n",
    "3. Experiments\n",
    "    - 3.1 Comparing fixed and variable length tokenisation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Initialisation\n",
    "## 1.1 Imports and config\n",
    "Libraries used include:\n",
    "- `fastai` for access to the ULMFiT model API\n",
    "- `pandas` and `matplotlib` for data science toolset\n",
    "\n",
    "Handy tricks include:\n",
    "- `autoreload` functionality so that updates to library code are automatically recompiled any time a cell is run.\n",
    "- `InteractiveShell` is used for multiple outputs from a single cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "import sentencepiece as spm\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import cupy as np\n",
    "\n",
    "# Imports from GenomicULMFiT repo\n",
    "sys.path.append(\"../../../Genomic-ULMFiT/\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from src.processing import process_fasta\n",
    "from src.config import GRCH38_P13, GENOME\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_LOCAL  = '/home/jovyan/ml_genomics'\n",
    "HOME_REMOTE = '/home/athon/'\n",
    "\n",
    "HOME        = HOME_REMOTE\n",
    "\n",
    "THESIS      = HOME / Path('thesis/')\n",
    "HUMAN       = THESIS / Path('data/human/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset params\n",
    "NROWS_TRAIN     = 10000\n",
    "NROWS_VAL       = 10000\n",
    "BATCH_SIZE      = 100\n",
    "\n",
    "# Tokenisation - fixed\n",
    "NGRAM_STRIDE    = [(3,1),(5,3),(7,5)]  #(ngram,stride) combinations for tokenisation\n",
    "\n",
    "# Tokenisation - variable\n",
    "MAX_VOCAB       = [4**3, 4**5, 4**7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Initialisation\n",
    "This process assumes data has already been read and processed into `.csv` format via the `process_fasta` script.\n",
    "Data is read from `human_genome_data_fa.csv` in chunks to reduce data in memory. This can then be iterated with `df.next()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter = pd.read_csv(HUMAN/'human_genome_data_fa.csv', chunksize=NROWS_TRAIN+NROWS_VAL)\n",
    "df = next(df_iter)\n",
    "\n",
    "# set val to be first 20k rows\n",
    "df_tr = df[:NROWS_TRAIN]\n",
    "df_va = df[NROWS_TRAIN:NROWS_TRAIN+NROWS_VAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Tokenisation\n",
    "## 2.1 Fixed length tokenisation\n",
    "- Technique of original author was to iterate over entire genome sequence with for loops.\n",
    "- We need to vectorise this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tools.tokenizers import seq_tokenizer, vec_tokenizer\n",
    "\n",
    "def seq_tokenizer(t, ngram, stride):\n",
    "    import time\n",
    "    t = t.upper()\n",
    "    if ngram == 1:\n",
    "        toks = list(t)\n",
    "    else:\n",
    "        start = time.time()\n",
    "        toks = [t[i:i+ngram] for i in range(0, len(t), stride) if len(t[i:i+ngram]) == ngram]\n",
    "\n",
    "    if len(toks[-1]) < ngram:\n",
    "        toks = toks[:-1]\n",
    "\n",
    "    return toks,time.time() - start\n",
    "\n",
    "def vec_tokenizer(a, ngram=3, stride=1, padnum=0):\n",
    "    import time\n",
    "    start = time.time()\n",
    "    a = np.array(list(a)).astype(object)\n",
    "    n = a.strides[0]\n",
    "    nrows = ((a.size)//stride)\n",
    "    strided = np.lib.stride_tricks.as_strided\n",
    "    out = strided(a[(ngram-1):-(stride-1)], shape=(nrows,ngram), strides=(stride*n,-n))[:,::-1] \n",
    "    out = out[ngram-1:,:]\n",
    "    toks = list(out[:,0] + out[:,1] + out[:,2])\n",
    "    return toks,time.time() - start\n",
    "\n",
    "# Test how well the vectorisation works\n",
    "def test_tokenizers():\n",
    "    seq_=[]; vec_=[]\n",
    "    for i in [1,10,100,1000,10000]:\n",
    "        test = ''.join([''.join(row) for row in df.head(i).values[:,0]])\n",
    "        toks ,time = seq_tokenizer(test, 3, 1);seq_.append(time)\n",
    "        toks_,time_ = vec_tokenizer(test, 3, 1);vec_.append(time_)\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "    stats=pd.DataFrame({'seq':seq_,'vec':vec_})\n",
    "    stats.plot(ax=ax)\n",
    "    ax.set_title('Sequential vs Vectorised Tokenisation')\n",
    "    ax.set_ylabel('Time')\n",
    "    \n",
    "test_tokenizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorised technique for faster fixed Length tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class FixedLengthGenomicTokenizer(BaseTokenizer):\n",
    "    \"\"\"\n",
    "    Fixed length tokenisation for DNA.\n",
    "    \"\"\"\n",
    "    def __init__(self, lang='en', ngram=5, stride=2):\n",
    "        self.lang = lang\n",
    "        self.ngram = ngram\n",
    "        self.stride = stride\n",
    "        \n",
    "    def tokenizer(self, t):\n",
    "        t = t.upper()\n",
    "        if self.ngram == 1:\n",
    "            toks = list(t)\n",
    "        else:\n",
    "            toks = [t[i:i+self.ngram] for i in range(0, len(t), self.stride) if len(t[i:i+self.ngram]) == self.ngram]\n",
    "        if len(toks[-1]) < self.ngram:\n",
    "            toks = toks[:-1]\n",
    "        return toks\n",
    "    \n",
    "    def add_special_cases(self, toks):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fixed_vocab(df_train, df_val, ngram=3, stride=1):\n",
    "    \"\"\"Create fixed length tokenizer, initialise databunch and return vocabulary.\"\"\"\n",
    "    \n",
    "    # initialise tokeniser\n",
    "    tok = Tokenizer(partial(GenomicTokenizer, ngram=ngram, stride=stride), \n",
    "                    n_cpus=40,\n",
    "                    pre_rules=[],\n",
    "                    post_rules=[],\n",
    "                    special_cases=[])\n",
    "    \n",
    "    data = GenomicTextLMDataBunch.from_df(HUMAN, df_train, df_val, bs=BATCH_SIZE, tokenizer=tok, \n",
    "                              chunksize=NROWS_TRAIN, text_cols=0, label_cols=1, max_vocab=((4**ngram)+1))\n",
    "\n",
    "    # Save and load vocab\n",
    "    np.save(HUMAN / 'fixed_vocab_{}m{}s.npy'.format(ngram,stride), data.vocab.itos)\n",
    "    return data.vocab.itos\n",
    "    \n",
    "vocabs = []\n",
    "for ngram_stride in NGRAM_STRIDE:\n",
    "    vocabs.append(generate_fixed_vocab(df_tr, df_va, *ngram_stride))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_variable_vocab(df_train, df_val, size=128):\n",
    "    \"\"\"Create variable length vocabulary using SentencePiece tokenisation.\n",
    "    \"\"\"\n",
    "    sp_proc = SPProcessor(char_coverage=1, \n",
    "                          vocab_sz=size,\n",
    "                          n_cpus = 40,\n",
    "                          pre_rules=[],\n",
    "                          post_rules=[])\n",
    "    \n",
    "    data = GenomicTextLMDataBunch.from_df(\n",
    "        HUMAN, df_train, df_val, bs=BATCH_SIZE, processor=sp_proc,\n",
    "        chunksize=NROWS_TRAIN, text_cols=0, label_cols=1, max_vocab=size\n",
    "    )\n",
    "    \n",
    "    # Save and load vocab\n",
    "    np.save(HUMAN / 'variable_vocab_{}tok.npy'.format(size), data.vocab.itos)\n",
    "    return data.vocab.itos\n",
    "    \n",
    "for max_vocab in MAX_VOCAB:\n",
    "    vocabs.append(generate_variable_vocab(df_tr, df_va, size=max_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(vocabs).T.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Setup\n",
    "We propose to explore model performance across different fixed and variable token parameters.\n",
    "For fixed length tokenisation, the parameters of interest are the number of base pairs per token, also known as `ngram`.\n",
    "Additionally, the number of base pairs between the start of 1 token and the next is known as `stride`. As an example, take the sequence of base pairs:\n",
    "$$\\text{TCTGGCGACAACCAGGGA}$$\n",
    "\n",
    "Using fixed length tokenisations of size **_3_** and stride **_0_**, we have the following outputs:\n",
    "$$\\text{[TCT],[GGC],[GAC],[AAC],[CAG],[GGA]}$$\n",
    "\n",
    "For parameters: `{size:3, stride:1}`\n",
    "$$\\text{[TCT],[],[],[GGC],[],[],[GAC],[],[],[AAC],[],[],[CAG],[],[],[GGA]}$$\n",
    "\n",
    "For parameters: `{size:5, stride:3}`\n",
    "$$\\text{[TCT],[GGC],[GAC],[AAC],[CAG],[GGA]}$$\n",
    "\n",
    "For parameters: `{size:7, stride:5}`\n",
    "$$\\text{[TCT],[GGC],[GAC],[AAC],[CAG],[GGA]}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating colour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML as html_print\n",
    "from pylab import *\n",
    "\n",
    "def printc(s, color='white'):\n",
    "    display(html_print(\"<text style=color:{}>{}</text>\".format(color, s)))\n",
    "\n",
    "def colour_gradient(n):\n",
    "    cmap = cm.get_cmap('viridis', n)    # PiYG\n",
    "\n",
    "    hexcol = []\n",
    "    for i in range(cmap.N):\n",
    "        rgb = cmap(i)[:3] # will return rgba, we take only first 3 so we get rgb\n",
    "        c_hex = matplotlib.colors.rgb2hex(rgb)\n",
    "        printc('test', str(c_hex))\n",
    "        \n",
    "def construct_html(wordlist, newline=50):\n",
    "    html=\"\"\n",
    "    for i,pair in enumerate(wordlist):\n",
    "        if (i!=0 and i%newline == 0):\n",
    "            display(html_print(html))\n",
    "            html=\"\"\n",
    "        html = html + \"<text style=font-family:monospace;color:{1}>{0}</text>\".format(*pair)\n",
    "    display(html_print(html))\n",
    "\n",
    "def hexcols(n=100):\n",
    "    cmap = cm.get_cmap('viridis', n)\n",
    "    return [matplotlib.colors.rgb2hex(cmap(i)[:3]) for i in range(cmap.N)]\n",
    "\n",
    "# variable length tokenised data\n",
    "i = 10; test = ''.join([''.join(row) for row in df.head(i).values[:,0]])\n",
    "test = test[:1600]\n",
    "construct_html([x for x in zip(test,hexcols(len(test)))], newline=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise data\n",
    "def tok_data(df_train, df_val, tok='Fixed', params=None):\n",
    "    \"\"\"Tokenise train and val data with provided tokenisation technique and parameters.\n",
    "    \"\"\"\n",
    "    if tok == 'Fixed':\n",
    "        ngram,stride = params\n",
    "        tok = Tokenizer(partial(GenomicTokenizer, ngram=ngram, stride=stride), n_cpus=40,\n",
    "                        pre_rules=[], post_rules=[], special_cases=[])\n",
    "        data = GenomicTextLMDataBunch.from_df(HUMAN, df_train, df_val, bs=BATCH_SIZE, tokenizer=tok, \n",
    "                              chunksize=NROWS_TRAIN, text_cols=0, label_cols=1, max_vocab=((4**ngram)+1))\n",
    "\n",
    "    elif tok == 'Variable':\n",
    "        size = params\n",
    "        sp_proc = SPProcessor(char_coverage=1,vocab_sz=size,n_cpus = 40,pre_rules=[],post_rules=[])\n",
    "        data = GenomicTextLMDataBunch.from_df(HUMAN, df_tr, df_va, bs=NROWS_TRAIN//10, \n",
    "                                              processor=sp_proc, chunksize=NROWS_TRAIN,\n",
    "                                              text_cols=0, label_cols=1, max_vocab=size)\n",
    "    return data\n",
    "\n",
    "test_sp = tok_data(df_tr, df_va, tok='Variable', params=(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_str = ''.join([str(test_sp.x[i]) for i in range(len(test_sp.x))])\n",
    "tokens  = [tok for tok in tok_str.split(' ') if set(tok) <= set('ACTG')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tok_dist(tokens, sample=50):\n",
    "    fig, axs = plt.subplots(nrows=3, figsize=(12,12))\n",
    "    \n",
    "    pd.Series(tokens).value_counts().tail(-1).plot(ax=axs[0],cmap='viridis')\n",
    "    axs[0].set_title('Token distribution.'.format(sample))\n",
    "    axs[0].set_ylabel('Count')\n",
    "    \n",
    "    pd.Series(tokens)\\\n",
    "        .value_counts().tail(-1).sample(sample).sort_values(ascending=False).plot.bar(ax=axs[1], cmap='viridis')\n",
    "    axs[1].set_title('MC Sample of Token Distribution, sample size {}.'.format(sample))\n",
    "    axs[1].set_ylabel('Count')\n",
    "    \n",
    "    pd.Series([len(token) for token in tokens])\\\n",
    "        .value_counts().sort_index().plot.bar(ax=axs[2], cmap='viridis')\n",
    "    axs[2].set_title('Token Length Distribution')\n",
    "    axs[2].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "plot_tok_dist(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colouring Bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(tokens).value_counts().tail(-1)\n",
    "def vis_tokens(toks=tokens[-1000:], by='length'):\n",
    "    \n",
    "    if by == 'length':\n",
    "        ys = [len(token) for token in toks]\n",
    "    \n",
    "    elif by == 'likelihood':\n",
    "        counts = pd.Series(tokens).value_counts()\n",
    "        ys = [counts[str(token)] if token != 'C' else 2000 for token in toks]\n",
    "        \n",
    "    ymax = max(ys); ymin = min(ys)\n",
    "    cmap = cm.get_cmap('viridis', ymax)\n",
    "\n",
    "    hexs = [matplotlib.colors.rgb2hex(cmap(y)[:3]) for y in ys]\n",
    "    construct_html([x for x in zip(toks,hexs)], newline=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colour by token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_tokens(by='length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colour by token likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_tokens(by='likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Experiment 1: Fixed Length Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = dict(emb_sz=400, \n",
    "                    n_hid=1150, \n",
    "                    n_layers=3, \n",
    "                    pad_token=0, \n",
    "                    qrnn=False, \n",
    "                    output_p=0.25, \n",
    "                    hidden_p=0.1, \n",
    "                    input_p=0.2, \n",
    "                    embed_p=0.02, \n",
    "                    weight_p=0.15, \n",
    "                    tie_weights=True, \n",
    "                    out_bias=True)\n",
    "\n",
    "DROP_MULT   = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = []\n",
    "\n",
    "# fixed length\n",
    "for ngram_stride in NGRAM_STRIDE:\n",
    "    experiment = {}\n",
    "    experiment['title'] = 'fixed_{}_{}'.format(*ngram_stride)\n",
    "#     experiment['vocab'] = generate_fixed_vocab(df_tr, df_va, *ngram_stride)\n",
    "    experiment['xdata'] = tok_data(df_tr, df_va, 'Fixed', ngram_stride)\n",
    "    \n",
    "    experiments.append(experiment)\n",
    "\n",
    "# variable length   \n",
    "for max_vocab in MAX_VOCAB:\n",
    "    experiment = {}\n",
    "    experiment['title'] = 'variable_{}'.format(max_vocab)    \n",
    "#     experiment['vocab'] = generate_variable_vocab(df_tr, df_va, max_vocab)\n",
    "    experiment['xdata'] = tok_data(df_tr, df_va, 'Variable', max_vocab)\n",
    "    \n",
    "    experiments.append(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('experiments.pkl', 'wb') as out:\n",
    "    pkl.dump(experiments, out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('experiments.pkl', 'rb') as infile:\n",
    "    experiments = pkl.load(infile)\n",
    "    infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(experiment, n_cycles=1, lr_find=False):\n",
    "    \n",
    "    config = dict(emb_sz=400, \n",
    "              n_hid=1150, \n",
    "              n_layers=3, \n",
    "              pad_token=0, \n",
    "              qrnn=False, \n",
    "              output_p=0.25, \n",
    "              hidden_p=0.1, \n",
    "              input_p=0.2, \n",
    "              embed_p=0.02, \n",
    "              weight_p=0.15, \n",
    "              tie_weights=True, \n",
    "              out_bias=True)\n",
    "    drop_mult=0.3\n",
    "    \n",
    "    data  = experiment['xdata']\n",
    "    learn = get_model_LM(data, drop_mult, config)\n",
    "    learn = learn.to_fp16(dynamic=True); # convert model weights to 16-bit float\n",
    "    \n",
    "    # add callbacks\n",
    "    from fastai.callbacks.csv_logger import CSVLogger\n",
    "    learn.callback_fns.append(partial(CSVLogger, append=True))\n",
    "    \n",
    "    if lr_find:\n",
    "        learn.lr_find()\n",
    "        learn.recorder.plot()\n",
    "    \n",
    "    learn.fit_one_cycle(n_cycles, 2e-2, moms=(0.8, 0.7))\n",
    "                \n",
    "    learn.save(experiment['title'])\n",
    "    learn.save_encoder(experiment['title'])\n",
    "    \n",
    "    # free up cuda\n",
    "    del learn; del data; torch.cuda.empty_cache()\n",
    "\n",
    "for experiment in experiments:\n",
    "    print(experiment['title'])\n",
    "    train_model(experiment, n_cycles=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freememory():\n",
    "    \"\"\"\n",
    "    Run garbage collection to free up memory.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "freememory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboardX\n",
    "from fastai.callbacks.tensorboard import LearnerTensorboardWriter\n",
    "from pathlib import Path\n",
    "\n",
    "project_id = 'exp1'\n",
    "tboard_path = Path('./logs/' + project_id)\n",
    "learn.callback_fns.append(partial(LearnerTensorboardWriter, \n",
    "                                    base_dir=tboard_path, \n",
    "                                    name='run1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GenomicTextLMDataBunch.from_df(path, df[20000:40000], df_val, bs=3000, tokenizer=tok, vocab=model_vocab, max_vocab=80000,\n",
    "                              chunksize=10000, text_cols=0, label_cols=1)\n",
    "\n",
    "config = dict(emb_sz=400, \n",
    "              n_hid=1150, \n",
    "              n_layers=3, \n",
    "              pad_token=0, \n",
    "              qrnn=False, \n",
    "              output_p=0.25, \n",
    "              hidden_p=0.1, \n",
    "              input_p=0.2, \n",
    "              embed_p=0.02, \n",
    "              weight_p=0.15, \n",
    "              tie_weights=True, \n",
    "              out_bias=True)\n",
    "\n",
    "drop_mult=0.3\n",
    "\n",
    "learn = get_model_LM(data, drop_mult, config)\n",
    "learn = learn.to_fp16(dynamic=True);\n",
    "\n",
    "learn.fit_one_cycle(2, 5e-3, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = np.load(path/'human_vocab_3m1s.npy')\n",
    "model_vocab = GenomicVocab(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "lr = 5e-3\n",
    "for df in df_iter:\n",
    "    data = GenomicTextLMDataBunch.from_df(path, df, df_val, bs=800, tokenizer=tok, vocab=model_vocab, max_vocab=80000,\n",
    "                                  chunksize=20000, text_cols=0, label_cols=1)\n",
    "    learn.data = data                        \n",
    "    lr_iter = lr/1.5**count\n",
    "    print(f'Learning Rate: {lr_iter}')\n",
    "    learn.fit_one_cycle(1, lr, moms=(0.8,0.7))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('human_3m1s2')\n",
    "learn.save_encoder('human_3m1s_enc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('human_3m1s2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.to_fp32();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('human_3m1s2_fp32')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-thesis] *",
   "language": "python",
   "name": "conda-env-.conda-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
